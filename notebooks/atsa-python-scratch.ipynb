{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:359: RuntimeWarning: invalid value encountered in less\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:361: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds  19.24906301498413\n",
      "finished cloud masking\n",
      "seconds  287.02473044395447\n",
      "finished potential shadow masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n",
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/ipykernel_launcher.py:569: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds  4.96546196937561\n",
      "finished refined shadow masking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rave/anaconda3/envs/geo/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: forest_cloud_and_shadow_masks.tif is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio import fill\n",
    "import skimage as ski\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api\n",
    "from sklearn.cluster import KMeans\n",
    "from math import ceil\n",
    "from skimage.draw import line\n",
    "from skimage.morphology import dilation, opening\n",
    "from skimage.filters import threshold_li\n",
    "import skimage.io as skio\n",
    "os.chdir(\"/home/rave/cloud-free-planet/atsa-python\")\n",
    "import pyatsa_configs\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "###porting code from original idl written by Xiaolin Zhu\n",
    "path_id = \"forest\"\n",
    "ATSA_DIR=\"/home/rave/cloud-free-planet/atsa-test-unzipped/\"\n",
    "img_path = \"/home/rave/cloud-free-planet/cfg/buffered_stacked/\"+ path_id+\"_stacked.tif\"\n",
    "angles_path = os.path.join(\"/home/rave/cloud-free-planet/cfg/buffered_angles\", path_id+'_angles_larger_utm.txt')\n",
    "configs = pyatsa_configs.ATSA_Configs(img_path, angles_path, ATSA_DIR)\n",
    "\n",
    "\n",
    "def map_processes(func, args_list):\n",
    "    \"\"\"\n",
    "    Set MAX_PROCESSES in preprocess_config.yaml\n",
    "    args_sequence is a list of lists of args\n",
    "    \"\"\"\n",
    "    processes = cpu_count()-1\n",
    "    pool = Pool(processes)\n",
    "    results = pool.starmap(func, args_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "\n",
    "#Computing the Clear Sky Line for Planet Images in T Series\n",
    "#Zhu set to 1.5 if it was less than 1.5 but this might not be a good idea for Planet \n",
    "#due to poorer calibration?\n",
    "def reject_outliers_by_med(data, m = 2.):\n",
    "    \"\"\"\n",
    "    Reject outliers based on median deviation\n",
    "    https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list\n",
    "    \"\"\"\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else 0.\n",
    "    return data[s<m].flatten()\n",
    "\n",
    "def reject_outliers_by_mean(data_red, data_blue, m = 3.):\n",
    "    \"\"\"\n",
    "    Reject outliers based on deviation from mean\n",
    "    This is the method used in Zhu and Elmer 2018\n",
    "    \"\"\"\n",
    "    return (data_red[data_red <= np.mean(data_red) + m * np.std(data_red)], \\\n",
    "            data_blue[data_red <= np.mean(data_red) + m * np.std(data_red)])\n",
    "\n",
    "def get_histo_labels(img, rmin0, rmax, nbins=50):\n",
    "    \"\"\"\n",
    "    Takes an image of shape [H, W, Channel], gets blue and red bands,\n",
    "    and computes histogram for blue values. then it finds the array indices\n",
    "    for each bin of the histogram.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy array): the input image, part of a time series of images\n",
    "        rmin0 (int): minimum edge of the histogram (later adjusted based on each image)\n",
    "        rmax (int): maximum edge of the histogram\n",
    "        nbins (int): number of histogram bins\n",
    "        \n",
    "    Returns:\n",
    "        the array of histogram indices of same shape as a band or (nan, nan)\n",
    "        which is later passed to compute hot when nanmean is called.\n",
    "    \n",
    "    \"\"\"\n",
    "    # make 3D arrays for blue and red bands to compute clear sky lines\n",
    "    blue = img[:,:,0]\n",
    "    red = img[:,:,2]\n",
    "    # finding samples, there should be at least 500 values to \n",
    "    # compute clear sky line\n",
    "    good_histo_values = np.where((blue<rmax)&(blue>rmin0), blue, 0)\n",
    "    if np.count_nonzero(good_histo_values) > 500:\n",
    "        rmin = np.min(good_histo_values[good_histo_values!=0]) # starts binning where we have good data\n",
    "        # computes the histogram for a single blue image\n",
    "        (means, edges, numbers)=stats.binned_statistic(blue.flatten(), \n",
    "                blue.flatten(), statistic='mean', \n",
    "                bins=50, range=(int(rmin),int(rmax)))\n",
    "        \n",
    "        histo_labels_reshaped = np.reshape(numbers, (blue.shape[0],blue.shape[1]))\n",
    "        return histo_labels_reshaped\n",
    "    \n",
    "    else:\n",
    "        # we return None here to signal that we need to use the \n",
    "        # mean slope and intercept for the good clear skylines\n",
    "        return np.ones(blue.shape)*np.nan\n",
    "    \n",
    "def get_bin_means(img, histo_labels_reshaped, n=20):\n",
    "    \"\"\"\n",
    "    Takes the same img as get_histo_labels and the histogram index array. \n",
    "    Only computes means for bins with at least n values and only takes the\n",
    "    highest n values in each bin to compute the mean. n is hardcoded to 20\n",
    "    in Zhu code.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy array): the input image, part of a time series of images\n",
    "        histo_labels_reshaped: array of same shape as the img bands\n",
    "        \n",
    "    Returns:\n",
    "        a tuple of two lists, the blue means and the read means\n",
    "    \"\"\"\n",
    "    blue = img[:,:,0]\n",
    "    red = img[:,:,2]\n",
    "\n",
    "    red_means=[]\n",
    "    blue_means=[]\n",
    "    # removing last element because for some reason there was an extra bin in the python version compared to idl\n",
    "    for i in np.unique(histo_labels_reshaped)[0:-1]:\n",
    "\n",
    "        red_vals = red[histo_labels_reshaped==i]\n",
    "        blue_vals = blue[histo_labels_reshaped==i]\n",
    "        # Zhu set this thresh for number of values needed in bin to compute mean\n",
    "        if len(blue_vals) >= n: \n",
    "            # before selecting top 20, reject outliers based on \n",
    "            # red values and pair with corresponding blue values as per Zhu code\n",
    "            (red_vals_no_outliers, blue_vals_no_outliers) = reject_outliers_by_mean(red_vals, blue_vals)\n",
    "\n",
    "            ## added these steps from Zhu code, but not sure if/why they are necessary\n",
    "            # they result in fewer values being averaged in each bin sometimes\n",
    "            # need to sort by red and use same sorting for blue to keep pairs together\n",
    "            sort_indices = np.argsort(red_vals_no_outliers)\n",
    "            red_vals_sorted = red_vals_no_outliers[sort_indices]\n",
    "            blue_vals_sorted = blue_vals_no_outliers[sort_indices]\n",
    "            select_n = min([n, ceil(.01*len(blue_vals))])\n",
    "            red_selected = red_vals_sorted[-select_n:]\n",
    "            blue_selected = blue_vals_sorted[-select_n:]\n",
    "            ##\n",
    "            #finds the highest red values and takes mean\n",
    "            red_means.append(\n",
    "                np.mean(\n",
    "                    red_selected\n",
    "                )\n",
    "            )\n",
    "            blue_means.append(\n",
    "                np.mean(\n",
    "                    blue_selected\n",
    "                )\n",
    "            )\n",
    "    return (blue_means, red_means)\n",
    "    \n",
    "    \n",
    "def get_intercept_and_slope(blue_means, red_means, histo_labels_reshaped, nbins):\n",
    "    \"\"\"\n",
    "    Takes the mean lists, the histogram labels, and nbins and computes the intercept\n",
    "    and slope. includes logic for dealing with too few bins and if the slope that\n",
    "    is computed is too low.\n",
    "    \n",
    "    Args:\n",
    "        blue_means (list): means of the bins for the blue band\n",
    "        red_means (list): means of the bins for the red band\n",
    "        histo_labels_reshaped: array of same shape as the img bands\n",
    "        \n",
    "    Returns:\n",
    "        a tuple of two floats, the intercept and the slope.\n",
    "    \"\"\"\n",
    "    # we want at least half of our ideal data points to construct the clear sky line\n",
    "    if len(np.unique(histo_labels_reshaped)) > .5 * nbins:\n",
    "        #followed structure of this example: https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html\n",
    "        model = statsmodels.formula.api.quantreg('reds~blues', {'reds':red_means, 'blues':blue_means})\n",
    "\n",
    "        result = model.fit()\n",
    "\n",
    "        intercept = result.params[0]\n",
    "        slope = result.params[1]\n",
    "        # mark as mean, later this is filled with mean slope and mean intercept\n",
    "        if slope < 1.5:\n",
    "            return (np.nan,np.nan)\n",
    "        return (intercept, slope)\n",
    "    # if we don't have even half the ideal amount of bin means...\n",
    "    # mark as mean, later this is filled with mean slope and mean intercept\n",
    "    else: \n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "def get_clear_skyline(img, rmin0, rmax, nbins=50):\n",
    "    \"\"\"\n",
    "    Computes the clear sky line for a single image using the\n",
    "    automatic bin based approach used by Zhen and Elmer 2018.\n",
    "    Returns the slope and intercept of the clear sky line.\n",
    "    Larger images are easier to compute a clear sky line, \n",
    "    smaller images with more clouds are more difficult and may\n",
    "    need to take an assumed slope or both slope and intercept.\n",
    "    This function puts the steps together. \n",
    "    \n",
    "    Args:\n",
    "        img (numpy array): bgrnir array\n",
    "        rmin0 (int): minimum edge of the histogram (later adjusted based on each image)\n",
    "        rmax (int): maximum edge of the histogram\n",
    "        nbins (int): number of histogram bins\n",
    "    \n",
    "    Returns:\n",
    "        tuple of nan if there are not enough good values to compute \n",
    "        a histogram\n",
    "        \n",
    "        or\n",
    "        \n",
    "        a tuple with the intercept and slope of the clear sky line.\n",
    "        See get_intercept_and_slope for logic on how intercept and slope\n",
    "        is computed with different edge cases\n",
    "    \"\"\"\n",
    "\n",
    "    histo_labels_reshaped = get_histo_labels(img, rmin0, rmax, nbins)\n",
    "    if np.isnan(histo_labels_reshaped).all() == True:\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    blue_means, red_means = get_bin_means(img, histo_labels_reshaped)\n",
    "    \n",
    "    intercept, slope = get_intercept_and_slope(blue_means, red_means, histo_labels_reshaped, nbins)\n",
    "    \n",
    "    return (intercept, slope)\n",
    "    \n",
    "def compute_hot_series(t_series, rmin, rmax, n_bin=50):\n",
    "    \"\"\"Haze Optimized Transformation (HOT) test\n",
    "    Equation 3 (Zhu and Woodcock, 2012)\n",
    "    Based on the premise that the visible bands for most land surfaces\n",
    "    are highly correlated, but the spectral response to haze and thin cloud\n",
    "    is different between the blue and red wavelengths.\n",
    "    Zhang et al. (2002)\n",
    "    In this implementation, the slope (a) and intercept(b)\n",
    "    of the clear sky line are computed automatically using a bin based approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_series: a 4D array with the band index as the third axis, image index as\n",
    "    the fourth axis (counting from 1st).\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    ndarray: The values of the HOT index for the image, a 3D array\n",
    "    \"\"\"\n",
    "    blues = t_series[:,:,0,:]\n",
    "    reds = t_series[:,:, 2,:]\n",
    "    intercepts_slopes = np.array(\n",
    "        list(map(lambda x: get_clear_skyline(x,rmin,rmax),\n",
    "                np.moveaxis(t_series,3,0)))\n",
    "        )\n",
    "    # assigns slope and intercept if an image is too cloudy (doesn't have 500 pixels in rmin, rmax range)\n",
    "    if np.isnan(intercepts_slopes).all():\n",
    "        # extreme case where no images can get a clear sky line\n",
    "        intercepts_slopes[:,1] = 1.5\n",
    "        intercepts_slopes[:,0] = 0\n",
    "    if np.isnan(intercepts_slopes).any():\n",
    "        # case where some images can't get a clear skyline\n",
    "        intercepts_slopes[:,1][np.isnan(intercepts_slopes[:,1])] = np.nanmean(intercepts_slopes[:,1])\n",
    "        intercepts_slopes[:,0][np.isnan(intercepts_slopes[:,0])] = np.nanmean(intercepts_slopes[:,0])\n",
    "    def helper(blue, red, ba):\n",
    "        b,a = ba\n",
    "        return abs(blue*a - red+b)/np.sqrt(1.0+a**2)\n",
    "    # map uses the first axis as the axis to step along\n",
    "    # need to use lambda to use multiple args\n",
    "    hot_t_series = np.array(list(map(lambda x,y,z: helper(x,y,z), \n",
    "                    np.moveaxis(blues,2,0), \n",
    "                    np.moveaxis(reds,2,0), \n",
    "                    intercepts_slopes)))\n",
    "    return hot_t_series, intercepts_slopes\n",
    "\n",
    "def reassign_labels(class_img, cluster_centers, k=3):\n",
    "    \"\"\"Reassigns mask labels of t series\n",
    "    based on magnitude of the cluster centers.\n",
    "    This assumes land will always be less than thin\n",
    "    cloud which will always be less than thick cloud,\n",
    "    in HOT units\"\"\"\n",
    "    idx = np.argsort(cluster_centers.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(k)\n",
    "    return lut[class_img]\n",
    "\n",
    "def sample_and_kmeans(hot_t_series, hard_hot=6000, sample_size=10000):\n",
    "    \"\"\"Trains a kmeans model on a sample of the time series\n",
    "    and runs prediction on the time series.\n",
    "    A hard coded threshold for the hot index, hard_hot, is\n",
    "    for allowing the kmeans model to capture more variation \n",
    "    throughout the time series. Without it, kmeans is skewed toward\n",
    "    extremely high HOT values and classifies most of the time series\n",
    "    as not cloudy.\"\"\"\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    # kmeans centers differ slightly due to the method of initialization.\n",
    "    # Zhu used mean and standard deviation fo the systematic sample, we use kmeans++\n",
    "    km = KMeans(n_clusters=3, n_init=1, max_iter=50, tol=1e-4, n_jobs=-1, \n",
    "                      verbose=False, random_state=4)\n",
    "\n",
    "#     sample_values = np.random.choice(\n",
    "#         hot_t_series.flatten()[hot_t_series.flatten()<hard_hot], \n",
    "#         size=sample_size).reshape(-1,1)\n",
    "    interval = int(len(hot_t_series.flatten())/sample_size)\n",
    "    sample_values = hot_t_series.flatten()[::interval].reshape(-1,1)\n",
    "    \n",
    "    fit_result = km.fit(sample_values)\n",
    "    \n",
    "    predicted_series = fit_result.predict(hot_t_series.flatten().reshape(-1,1)).reshape(hot_t_series.shape)\n",
    "    \n",
    "    return reassign_labels(predicted_series, fit_result.cluster_centers_, k=3), fit_result.cluster_centers_\n",
    "\n",
    "def calculate_upper_thresh(hot_t_series, cloud_masks, A_cloud):\n",
    "    \"\"\"Uses temporal refinement as defined by Zhu and Elmer 2018\n",
    "    to catch thin clouds by defining the upper boundary, U for clear \n",
    "    pixels. Later we might want to compute a neighborhood std \n",
    "    through the t_series.\"\"\"\n",
    "    hot_potential_clear = np.array(list(map(\n",
    "        lambda x, y: np.where(x>0, np.nan, y),\n",
    "        cloud_masks, hot_t_series))) # set cloud to nan\n",
    "    hot_potential_cloudy = np.array(list(map(\n",
    "        lambda x, y: np.where(x==0, np.nan, y),\n",
    "        cloud_masks, hot_t_series))) # set non cloud to nan\n",
    "    t_series_std = np.nanstd(hot_potential_clear, axis=0)\n",
    "    t_series_mean = np.nanmean(hot_potential_clear, axis=0)\n",
    "    t_series_min = np.nanmin(hot_potential_clear, axis=0)\n",
    "    t_series_max = np.nanmax(hot_potential_clear, axis=0)\n",
    "    range_arr = t_series_max - t_series_min\n",
    "    \n",
    "    # cloud_series_min can be computed more efficiently using k means centers \n",
    "    # if a single k means model is used\n",
    "    # according to Zhu in personal communciation. This is done in IDL code\n",
    "    \n",
    "    # NRDI (adjust_T in the IDL code) is a problem here because the HOT indices \n",
    "    # vary a lot in the planet images. if we train a kmeans model for each image\n",
    "    # Th_initial will have a very low initial value, if we train one kmeans model\n",
    "    # then the model will produce innacurate initial masks because of extremely high\n",
    "    # HOT values. Need to find a work around.\n",
    "    \n",
    "    # the sticky point is how cloud_series_min is calculated. if it is the minimum\n",
    "    # of all cloudy areas calculated by multiple kmeans models, it is not correct for \n",
    "    # the whole t series\n",
    "    \n",
    "    cloud_series_min = np.nanmin(hot_potential_cloudy.flatten(), axis=0)\n",
    "    \n",
    "    NRDI = (cloud_series_min - range_arr)/(cloud_series_min + range_arr)\n",
    "    upper_thresh_arr = t_series_mean + (A_cloud+NRDI)*t_series_std\n",
    "    \n",
    "    return (upper_thresh_arr, hot_potential_clear, hot_potential_cloudy)\n",
    "\n",
    "def apply_upper_thresh(t_series, hot_t_series, upper_thresh_arr, initial_kmeans_clouds, hot_potential_clear, hot_potential_cloudy, dn_max):\n",
    "    \"\"\"Applies the masking logic to refine the initial cloud\n",
    "    masks from k-means using the global threshold and \n",
    "    upper threshold computed from the time series.\n",
    "    Returns a time series of refined masks where 2 is cloud and 1 is clear land.\"\"\"\n",
    "    \n",
    "    cloud_series_mean_global = np.nanmean(hot_potential_cloudy.flatten(), axis=0)\n",
    "    cloud_series_std_global = np.nanstd(hot_potential_cloudy.flatten(), axis=0)\n",
    "    global_cloud_thresh = cloud_series_mean_global - 1.0*cloud_series_std_global\n",
    "    # 0 is where hot is below upper threshold, 1 is above\n",
    "    #testing fix, misse dthat I need to use this thresh here to refine\n",
    "    #initial clouds by setting pixels to not cloudy if the HOT values are too low\n",
    "    #refine initial cloud\n",
    "    initial_kmeans_clouds_binary = np.where(initial_kmeans_clouds > 0, 2 , 1)\n",
    "    refined_masks = np.where(np.less(hot_potential_cloudy, upper_thresh_arr), 1, initial_kmeans_clouds_binary)\n",
    "    # add missed clouds\n",
    "    refined_masks = np.where(np.logical_and(np.greater(hot_potential_clear, upper_thresh_arr), reshape_as_raster(np.greater(t_series[:,:,2,:],dn_max*.1))), 2, refined_masks)\n",
    "    \n",
    "    global_thresh_arr = np.ones(refined_masks.shape)*global_cloud_thresh\n",
    "    \n",
    "    refined_masks = np.where(hot_t_series > global_cloud_thresh, 2, refined_masks)\n",
    "    \n",
    "    return refined_masks\n",
    "\n",
    "def cloud_height_min_max(angles, longest_d, shortest_d):\n",
    "    \"\"\"Calculates the range of possible cloud heights using the \n",
    "    scene metadata. The longest distance between a shadow and cloud\n",
    "    specified in the config cannot be larger than the number of rows \n",
    "    or columns in the image.\n",
    "    \n",
    "    Args:\n",
    "        angles (numpy array): 1st column is sun elevation, 2nd is azimuth \n",
    "    \"\"\"\n",
    "    angles = angles/180.0*3.1415926\n",
    "    h_high=longest_d/(((np.tan(angles[:,0])*np.sin(angles[:,1]))**2+(np.tan(angles[:,0])*np.cos(angles[:,1]))**2)**0.5)\n",
    "    h_low=shortest_d/(((np.tan(angles[:,0])*np.sin(angles[:,1]))**2+(np.tan(angles[:,0])*np.cos(angles[:,1]))**2)**0.5)\n",
    "    return h_high, h_low\n",
    "\n",
    "def cloud_height_ranges(h_high, h_low):\n",
    "    \"\"\"\n",
    "    Takes two arrays of the max cloud height and minimum cloud height, \n",
    "    returning a list of arrays the same length as the time series \n",
    "    containing the range of cloud heights used to compute the cloud shadow masks.\n",
    "    \n",
    "    Returns: Difference between heighest potential height and lowest, in pixel units.\n",
    "    \"\"\"\n",
    "    h_range_lengths = np.ceil((h_high-h_low)/3.0)\n",
    "    h_ranges = []\n",
    "    for i,x in enumerate(h_range_lengths):\n",
    "        h_ranges.append(np.arange(x)*3+h_low[i])\n",
    "    return h_ranges\n",
    "\n",
    "def shadow_shift_coords(h_ranges, angles):\n",
    "    \"\"\"\n",
    "    Computes the possible minimum and maximum x and y magnitudes and \n",
    "    directions (in a cartesian sense) for shadows for each scene based \n",
    "    on the scene geometry with the sun. Used to determine the direction of the shadow.\n",
    "    \n",
    "    Args:\n",
    "        h_ranges (list of numpy arrays): the ranges of cloud heights for \n",
    "            each scene, same length as time series\n",
    "        angles (numpy array): the sun elevation and azimuth angles. \n",
    "            column 0 is sun elevation, 1 is azimuth\n",
    "    Returns:\n",
    "        The ending x and y direction and magnitude of the \n",
    "            potential shadow relative to the cloud mask\n",
    "    \"\"\"\n",
    "    angles = angles/180.0*3.1415926\n",
    "    end_x1s = []\n",
    "    end_y1s = []\n",
    "    for i, heights in enumerate(h_ranges):      \n",
    "        end_x1s.append(int(round(-heights[-1]*np.tan(angles[i,0])*np.sin(angles[i,1]))))\n",
    "        end_y1s.append(int(round(heights[-1]*np.tan(angles[i,0])*np.cos(angles[i,1]))))\n",
    "    return list(zip(end_x1s, end_y1s))\n",
    "\n",
    "def make_rectangular_struct(shift_coord_pair):\n",
    "    \"\"\"\n",
    "    Makes the rectangular array with the line structure for dilation int he cloud shadow direction.\n",
    "    Expects the ending x and y coordinate in array index format for the maximal cloud shadow at the\n",
    "    maximal cloud height. Array index format means positive y indicates the shadow is south of the cloud, \n",
    "    positive x means the shadow is more east of the cloud. rr and cc are are intermediate arrays that store \n",
    "    the indices of the line. This line will run from the center of the struct to a corner of the array that \n",
    "    is opposite from the direction of the dilation.\n",
    "    \n",
    "    Args:\n",
    "        shift_coord_pair (tuple): Contains the following\n",
    "            shift_x (int): The maximum amount of pixels to shift the cloud mask in the x direction\n",
    "            shift_y (int): The maximum amount of pixels to shift the cloud mask in the y direction\n",
    "    Returns: The struct used by the skimage.morphology.dilation to get the potential shadow mask for a single\n",
    "                image.\n",
    "        \n",
    "    \"\"\"\n",
    "    shift_x, shift_y = shift_coord_pair\n",
    "    struct = np.zeros((abs(shift_y)*2+1, abs(shift_x)*2+1))\n",
    "    \n",
    "    if shift_x < 0 and shift_y < 0:\n",
    "        rr, cc = line(int(abs(shift_y)),int(abs(shift_x)), abs(shift_y)*2, abs(shift_x)*2)  \n",
    "    elif shift_x < 0 and shift_y > 0:\n",
    "        rr, cc = line(int(abs(shift_y)),int(abs(shift_x)), 0, abs(shift_x)*2)\n",
    "    elif shift_x > 0 and shift_y > 0:\n",
    "        rr, cc = line(int(abs(shift_y)),int(abs(shift_x)), 0, 0)   \n",
    "    elif shift_x > 0 and shift_y < 0:\n",
    "        rr, cc = line(int(abs(shift_y)),int(abs(shift_x)), abs(shift_y)*2, 0)   \n",
    "    struct[rr,cc] = 1\n",
    "    # removes columns and rows with only zeros, doesn't seem to have an affect\n",
    "    # struct = struct[~np.all(struct == 0, axis=1)]\n",
    "    # struct = struct[:, ~np.all(struct == 0, axis=0)]\n",
    "    return struct\n",
    "\n",
    "def potential_shadow(struct, cloud_mask):\n",
    "    \"\"\"\n",
    "    Makes the shadow mask from the struct and the cloud mask\n",
    "    \"\"\"\n",
    "    d = dilation(cloud_mask==2, selem=struct)\n",
    "    d = np.where(d==1, 0, 1)\n",
    "    d = np.where(cloud_mask==2, 2, d)\n",
    "    return d\n",
    "\n",
    "def make_potential_shadow_masks(shift_coords, cloud_masks):\n",
    "    structs = []\n",
    "    \n",
    "    for i in shift_coords:\n",
    "        structs.append(make_rectangular_struct(i))\n",
    "        \n",
    "    shadow_masks = list(map(lambda x, y: potential_shadow(x,y), structs, cloud_masks))\n",
    "    return np.stack(shadow_masks, axis=0), structs\n",
    "\n",
    "def make_potential_shadow_masks_multi(shift_coords, cloud_masks):\n",
    "    args_list = []\n",
    "    \n",
    "    for i,coord in enumerate(shift_coords):\n",
    "        args_list.append((make_rectangular_struct(coord),cloud_masks[i]))\n",
    "        \n",
    "    shadow_masks = list(map_processes(potential_shadow, args_list))\n",
    "    return np.stack(shadow_masks, axis=0)\n",
    "\n",
    "\n",
    "def apply_li_threshold_multi(shadow_inds, potential_shadow_masks):\n",
    "    args_list = list(zip(shadow_inds,potential_shadow_masks))\n",
    "    refined_shadow_masks = list(map_processes(apply_li_threshold, args_list))\n",
    "    return np.stack(refined_shadow_masks, axis=0)\n",
    "\n",
    "def min_cloud_nir(masks, t_series):\n",
    "    \"\"\"\n",
    "    Gets the nir band of the scene with the minimum amount of cloud. \n",
    "    This will need to be reworked to handle partial scenes so that\n",
    "    only full scenes are used.\n",
    "    \n",
    "    Args:\n",
    "        masks (numpy array): a 3D array of masks of shape \n",
    "                        (count, height, width)\n",
    "        t_series (numpy array): array of shape (height, width, bands, count) ordered RGBNIR\n",
    "    Returns (tuple): the nir band of the scene with the least clouds and the index of this scene in the t series\n",
    "    \"\"\"\n",
    "    assert np.unique(masks[0])[-1] == 2\n",
    "    cloud_counts = [(i==2).sum() for i in masks]\n",
    "    min_index = np.argmin(cloud_counts)\n",
    "    return t_series[:,:,3,min_index], min_index  # 3 is NIR\n",
    "\n",
    "def gain_and_bias(potential_shadow_masks, nir, clearest_land_nir, clearest_index, nir_index):\n",
    "    \"\"\"\n",
    "    Calculates gain for a single imag ein the time series relative to the clearest land image\n",
    "        Args:\n",
    "            potential_shadow_masks (numpy array): masks of shape (count, height, width) where 0 is shadow, 1 is clear, 2 is cloud\n",
    "            nir (numpy array): nir band of the scene to compute gain and bias for\n",
    "            clearest_land_nir: nir band of the clearest scene\n",
    "            clearest_index: index for clearest nir band, used for filtering with masks\n",
    "            nir_index: index for the other nir band\n",
    "        Returns (tuple): (gain, bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    # index 3 is NIR, 1 in the mask is clear land\n",
    "    both_clear = (potential_shadow_masks[clearest_index]==1) & (potential_shadow_masks[nir_index]==1)\n",
    "    if both_clear.sum() > 100:\n",
    "        clearest = clearest_land_nir[both_clear]\n",
    "        nir = nir[both_clear]\n",
    "        gain = np.std(clearest)/np.std(nir)\n",
    "        bias  = np.mean(clearest) - np.mean(nir) * gain\n",
    "    else:\n",
    "        gain = 1\n",
    "        bias = 0\n",
    "    return gain, bias\n",
    "\n",
    "def gains_and_biases(potential_shadow_masks, t_series, clear_land_nir, clear_land_index):\n",
    "    gains_biases = []\n",
    "    for i in np.arange(t_series.shape[-1]):\n",
    "        gain, bias = gain_and_bias(potential_shadow_masks, t_series[:,:,3,i], clear_land_nir, clear_land_index, i)\n",
    "        gains_biases.append((gain,bias))\n",
    "    return gains_biases\n",
    "\n",
    "def shadow_index_land(potential_shadow_masks, t_series, gains_biases):\n",
    "    \"\"\"\n",
    "    Applies gain and bias to get shadow index from nir for each scene in t series.\n",
    "    \n",
    "    Returns (numpy array): shape (count, height, width) of the nir band shadow index \n",
    "                where there was previously calculated to be potential shadow\n",
    "    \"\"\"\n",
    "    shadow_inds = []\n",
    "    for i in np.arange(t_series.shape[-1]):\n",
    "        # applies calcualtion only where mask says there is not cloud \n",
    "        # might need to do this differently for water\n",
    "        shadow_inds.append(np.where(potential_shadow_masks[i]!=2, t_series[:,:,3,i]*gains_biases[i][0]+gains_biases[i][1], np.nan))\n",
    "        \n",
    "    return np.stack(shadow_inds)\n",
    "\n",
    "def apply_li_threshold(shadow_index, potential_shadow_mask):\n",
    "    \"\"\"\n",
    "    Applies a Li threshold to the cloud masked shadow index\n",
    "    and subsets this binarized thresholded array to the first \n",
    "    potential shadow mask, refining potential shadow regions.\n",
    "    \n",
    "    skimage.filters.try_all_threshold showed that Li's threshold was far superior\n",
    "    to Otsu and other methods. This replaces IDL's use of Inverse Distance Weighting \n",
    "    to refine the shadow mask before kmeans clustering since it is faster and returns better results.\n",
    "    \n",
    "    Args:\n",
    "        shadow_index (numpy array): output from shadow_index_land for a single scene that has clouds set to NaN\n",
    "        potential_shadow_mask (numpy array): the shadow and cloud mask, used to refine the thresholded mask\n",
    "    \n",
    "    https://www.sciencedirect.com/science/article/pii/003132039390115D?via%3Dihub\n",
    "    \"\"\"\n",
    "\n",
    "    thresh = threshold_li(shadow_index)\n",
    "\n",
    "    binary = shadow_index > thresh\n",
    "\n",
    "    binary = np.where(potential_shadow_mask==0, binary, 1)\n",
    "\n",
    "    return opening(binary)\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "angles = np.genfromtxt(angles_path, delimiter=' ')\n",
    "\n",
    "hot_t_series, intercepts_slopes = compute_hot_series(configs.t_series, configs.rmin, configs.rmax)\n",
    "\n",
    "initial_kmeans_clouds, kmeans_centers = sample_and_kmeans(hot_t_series, hard_hot=5000, sample_size=10000)\n",
    "\n",
    "upper_thresh_arr, hot_potential_clear, hot_potential_cloudy = calculate_upper_thresh(hot_t_series, initial_kmeans_clouds, configs.A_cloud)\n",
    "\n",
    "refined_masks = apply_upper_thresh(configs.t_series, hot_t_series, upper_thresh_arr, initial_kmeans_clouds, hot_potential_clear, hot_potential_cloudy, configs.dn_max)\n",
    "\n",
    "# axis 0 must be the image count axis, not height or width\n",
    "# refined_masks = np.apply_along_axis(opening, 0, refined_masks) # removes single pixel clouds\n",
    "\n",
    "# refined_masks = np.apply_along_axis(lambda x: dilation(x, selem=np.ones(5,5)), 0, refined_masks)\n",
    "\n",
    "for i in np.arange(refined_masks.shape[0]):\n",
    "    refined_masks[i] = opening(refined_masks[i])\n",
    "    refined_masks[i] = dilation(refined_masks[i], np.ones((5,5)))\n",
    "\n",
    "print(\"seconds \", time.time()-start)\n",
    "print(\"finished cloud masking\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "h_high, h_low = cloud_height_min_max(angles, configs.longest_d, configs.shortest_d)\n",
    "h_ranges = cloud_height_ranges(h_high, h_low)\n",
    "shift_coords = shadow_shift_coords(h_ranges, angles)\n",
    "potential_shadow_masks = make_potential_shadow_masks_multi(shift_coords, refined_masks)\n",
    "\n",
    "print(\"seconds \", time.time()-start)\n",
    "print(\"finished potential shadow masking\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clearest_land_nir, clearest_land_index = min_cloud_nir(potential_shadow_masks, configs.t_series)\n",
    "gains_biases = gains_and_biases(potential_shadow_masks, configs.t_series, clearest_land_nir, clearest_land_index)\n",
    "shadow_inds = shadow_index_land(potential_shadow_masks, configs.t_series, gains_biases)\n",
    "li_refined_shadow_masks = apply_li_threshold_multi(shadow_inds, potential_shadow_masks)\n",
    "cloud_shadow_masks = np.where(li_refined_shadow_masks==0,0,refined_masks) #2 is cloud, 1 is clear land, 0 is shadow\n",
    "\n",
    "skio.imsave(path_id+\"_cloud_and_shadow_masks.tif\",cloud_shadow_masks)\n",
    "print(\"seconds \", time.time()-start)\n",
    "print(\"finished refined shadow masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, ..., 2, 2, 2],\n",
       "        [1, 1, 1, ..., 2, 2, 2],\n",
       "        [1, 1, 1, ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]],\n",
       "\n",
       "       [[2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2]],\n",
       "\n",
       "       [[2, 2, 2, ..., 1, 1, 1],\n",
       "        [2, 2, 2, ..., 1, 1, 1],\n",
       "        [2, 2, 2, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [2, 2, 2, ..., 2, 1, 1],\n",
       "        [2, 2, 2, ..., 2, 1, 1],\n",
       "        [2, 2, 2, ..., 2, 1, 1]],\n",
       "\n",
       "       [[2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2]],\n",
       "\n",
       "       [[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as io\n",
    "state = io.readsav(\"/home/rave/tana-crunch/waves/cloud-free-planet/atsa-python/idl-shadow-variables.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking why kmeans centers are not super similar, probably due to the initialization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, cluster_centers = sample_and_kmeans(hot_t_series, hard_hot=6000, sample_size=10000)\n",
    "\n",
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state['center_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(51):\n",
    "    plt.figure()\n",
    "    skio.imshow(initial_kmeans_clouds[i,:,:])\n",
    "    plt.figure()\n",
    "    skio.imshow(configs.t_series[:,:,0,i])\n",
    "    plt.figure()\n",
    "    plt.title(\"idl refined masks\")\n",
    "    skio.imshow(state['kmeans_masks'][i])\n",
    "    plt.figure()\n",
    "    plt.title(\"refined masks\")\n",
    "    skio.imshow(refined_masks[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(initial_kmeans_clouds_binary[32,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(state['kmeans_masks'][32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state['g_th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_series_mean_global = np.nanmean(hot_potential_cloudy.flatten(), axis=0)\n",
    "cloud_series_std_global = np.nanstd(hot_potential_cloudy.flatten(), axis=0)\n",
    "global_cloud_thresh = cloud_series_mean_global - 1.0*cloud_series_std_global\n",
    "global_cloud_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large difference between hot image occurs for some images without clouds. error should lie between 77 and 97 lines in atsa.pro because the bin calculation is the same, intercepts and slopes are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(hot_t_series[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(state['HOT'][0,:,:], hot_t_series[0,:,:], atol=.01, rtol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skio.imshow(refined_masks[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes on IDL code follow the values to determine what conditions to use in python\n",
    "\n",
    "The mask values are as follows (everything starts as 1) and water mask is 0 value where water\n",
    "\n",
    "* 3 - background/SLC errors, missing data\n",
    "* 2 - cloud (see lines 365 through 378)\n",
    "* 1 - clear land (see lines 323 through 331, where idl returns 1 or 0 from ge condition)\n",
    "* 0 - shadow\n",
    "* 99 - shadow edge, removed because it produces artifacts in Planet imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making presentation figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_imgs_with_meta(img_paths, meta_paths, DIR):\n",
    "    imgs_with_meta = []\n",
    "    for im in list(map(os.path.basename, sorted(img_paths))):\n",
    "        for meta in list(map(os.path.basename, sorted(meta_paths))):\n",
    "            if meta[0:15] == im[0:15]:\n",
    "                imgs_with_meta.append(im)\n",
    "    return [os.path.join(DIR,name) for name in imgs_with_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udm_pattern = \"/home/rave/cloud-free-planet/notebooks/jan-september/*udm*.tif\"\n",
    "udm_paths = glob.glob(udm_pattern)\n",
    "udm_paths = sorted(udm_paths)\n",
    "meta_pattern = \"/home/rave/cloud-free-planet/notebooks/jan-september/*metadata.json\"\n",
    "meta_paths = glob.glob(meta_pattern)\n",
    "meta_paths = sorted(meta_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udm_with_meta = keep_imgs_with_meta(udm_paths, meta_paths, \"/home/rave/cloud-free-planet/notebooks/jan-september/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [ski.io.imread(path) for path in udm_with_meta]\n",
    "udms_stacked = reshape_as_raster(np.dstack(arrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udms_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "i = 0\n",
    "while i<20:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"HOT Index Mask (Partial ATSA)\")\n",
    "#     plt.title(\"Intercept:\"+str(intercepts_slopes[i][0])+\n",
    "#               \" and Slope:\"+str(intercepts_slopes[i][1]))\n",
    "    ski.io.imshow(cloud_masks[i,:,:])\n",
    "    plt.savefig(str(i)+\"atsa-k.png\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"UDM Mask (Planet)\")\n",
    "    ski.io.imshow(udms_stacked[i,:,:])\n",
    "    plt.savefig(str(i)+\"udm.png\")\n",
    "#     ski.io.imshow(hot_t_series[i,:,:])\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"True Color\")\n",
    "    ski.io.imshow(reorder_to_rgb(t_series[:,:,:,i]))\n",
    "    plt.savefig(str(i)+\"true-color.png\")\n",
    "    \n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.draw import rectangle, line\n",
    "from skimage.morphology import dilation\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as skio\n",
    "\n",
    "img = np.zeros((100, 100), dtype=np.uint8)\n",
    "start = (50, 50)\n",
    "extent = (10, 10)\n",
    "rr, cc = rectangle(start, extent=extent, shape=img.shape)\n",
    "img[rr, cc] = 1\n",
    "plt.figure()\n",
    "plt.title(\"fake square cloud\")\n",
    "skio.imshow(img)\n",
    "\n",
    "test = [[31,33],[31,-33],[-31,-33],[-31,33]]\n",
    "for t in test:\n",
    "    plt.figure()\n",
    "    plt.title(\"fake square shadow struct for a particular direction\")\n",
    "    skio.imshow(make_rectangular_struct(t))\n",
    "    plt.figure()\n",
    "    skio.imshow(dilation(img, selem=make_rectangular_struct(t)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo]",
   "language": "python",
   "name": "conda-env-geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
