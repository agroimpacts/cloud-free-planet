{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## package ideas \n",
    "- have a command that creates config file, have option to suppluy default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio import fill\n",
    "import skimage as ski\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api\n",
    "from skimage import exposure\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import morphology as morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# below are plotting and vis functions\n",
    "def percentile_rescale(arr, plow=1, phigh=99):\n",
    "    '''\n",
    "    Rescales and applies other exposure functions to improve image vis. \n",
    "    http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.rescale_intensity\n",
    "    '''\n",
    "    rescaled_arr = np.zeros_like(arr)\n",
    "    for i in range(0,arr.shape[-1]):\n",
    "        val_range = (np.percentile(arr[:,:,i], plow), np.percentile(arr[:,:,i], phigh))\n",
    "        rescaled_channel = exposure.rescale_intensity(arr[:,:,i], val_range)\n",
    "        rescaled_arr[:,:,i] = rescaled_channel\n",
    "#     rescaled_arr= exposure.adjust_gamma(rescaled_arr, gamma=1) #adjust from 1 either way\n",
    "#     rescaled_arr= exposure.adjust_sigmoid(rescaled_arr, cutoff=.50) #adjust from .5 either way \n",
    "    return rescaled_arr\n",
    "def normalize(arr):\n",
    "    ''' Function to normalize an input array to 0-1 '''\n",
    "    arr_max = arr.max()\n",
    "    return arr / arr_max\n",
    "\n",
    "def reorder_to_rgb(image):\n",
    "    '''reorders  bands ordered like BGRNIR\n",
    "    to blue, red, green for imshow\n",
    "    '''\n",
    "    blue = normalize(image[:,:,0])\n",
    "    green = normalize(image[:,:,1])\n",
    "    red = normalize(image[:,:,2])\n",
    "    nir = normalize(image[:,:,3])\n",
    "    return np.stack([red, green, blue], axis=-1) \n",
    "\n",
    "def plot_series(series, n, title, save=False):\n",
    "    \"\"\"\n",
    "    Plots n number of images in a series with shape\n",
    "    [number of images, rows, columns].\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < n:\n",
    "\n",
    "        plt.figure()\n",
    "        ski.io.imshow(series[i,:,:])\n",
    "        plt.title(title+': image '+str(i))\n",
    "        \n",
    "        if save ==True:\n",
    "            \n",
    "            plt.savefig(title+str(series[i,:,:].min())+'_imagemin_'+str(i)+\".png\")\n",
    "        i+=1\n",
    "\n",
    "###porting code from original idl written by Xiaolin Zhu\n",
    "    \n",
    "ATSA_DIR=\"/home/rave/tana-crunch/waves/cloud-free-planet/atsa-test-unzipped/\"\n",
    "img_path = os.path.join(ATSA_DIR, \"planet-pyatsa-test/stacked_larger_utm.tif\")\n",
    "img = ski.io.imread(img_path)\n",
    "#set the following parameters\n",
    "dn_max=10000  #maximum value of DN, e.g. 7-bit data is 127, 8-bit is 255\n",
    "tempfolder=os.path.join(ATSA_DIR, 'temp') # folder for storing intermediate results\n",
    "background=0  #DN value of background or missing values, such as SLC-off gaps\n",
    "buffer=1    #width of buffer applied to detected cloud and shadow, recommend 1 or 2 \n",
    "\n",
    "#parameters for HOT caculation and cloud detection\n",
    "#------------------------------\n",
    "n_band=4     # number of bands of each image\n",
    "n_image=img.shape[2]/n_band   # number of images in the time-series\n",
    "blue_b=0    # band index of blue band, note: MSS does not have blue, use green as blue\n",
    "green_b=1   # band index of green band\n",
    "red_b=2     # band index of red band\n",
    "nir_b=3     # band index of nir band\n",
    "\n",
    "A_cloud=0.5 # threshold to identify cloud (mean+A_cloud*sd), recommend 0.5-1.5, smaller values can detect thinner clouds\n",
    "maxblue_clearland=dn_max*0.15 # estimated maximum blue band value for clear land surface\n",
    "maxnir_clearwater=dn_max*0.05 # estimated maximum nir band value for clear water surface\n",
    "rmax = maxblue_clearland # max value for blue band for computing clear line\n",
    "rmin = .01*dn_max # min DN value for flue band for computing clear line\n",
    "n_bin = 50 # number of bins between rmin and rmax\n",
    "\n",
    "#parameters for shadow detection\n",
    "#------------------------------\n",
    "shortest_d=7.0       #shortest distance between shadow and cloud, unit is pixel resolution\n",
    "longest_d=50.0  #longest distance between shadow and its corresponding cloud, unit is \"pixel\",can be set empirically by inspecting images\n",
    "B_shadow=1.5   #threshold to identify shadow (mean-B_shadow*sd), recommend 1-3, smaller values can detect lighter shadows\n",
    "#------------------------------\n",
    "\n",
    "#we reshape our images that were stacked on the band axis into a 4D array\n",
    "t_series = np.reshape(img,(img.shape[0],img.shape[1],n_band,int(n_image)), order='F')\n",
    "\n",
    "#Computing the Clear Sky Line for Planet Images in T Series\n",
    "#Zhu set to 1.5 if it was less than 1.5 but this might not be a good idea for Planet \n",
    "#due to poorer calibration?\n",
    "def reject_outliers_by_med(data, m = 2.):\n",
    "    \"\"\"\n",
    "    Reject outliers based on median deviation\n",
    "    https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list\n",
    "    \"\"\"\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else 0.\n",
    "    return data[s<m].flatten()\n",
    "\n",
    "def get_clear_skyline(img, rmin, rmax, nbins=50):\n",
    "    \"\"\"\n",
    "    Computes the clear sky line for a single image using the\n",
    "    automatic bin based approach used by Zhen and Elmer 2018.\n",
    "    Returns the slope and intercept of the clear sky line.\n",
    "    Larger images are easier to compute a clear sky line, \n",
    "    smaller images with more clouds are more difficult and may\n",
    "    need to take an assumed slope or both slope and intercept.\n",
    "    \"\"\"\n",
    "    # make 3D arrays for blue and red bands to compute clear sky lines\n",
    "    blue = img[:,:,0]\n",
    "    red = img[:,:,2]\n",
    "    # finding samples, there should be at least 500 values to \n",
    "    # compute clear sky line\n",
    "    good_histo_values = np.where((blue<rmax)&(blue>rmin), blue, 0)\n",
    "    if np.count_nonzero(good_histo_values) > 500:\n",
    "        # computes the histogram for a single blue image\n",
    "        (means, edges, numbers)=stats.binned_statistic(blue.flatten(), \n",
    "                blue.flatten(), statistic='mean', \n",
    "                bins=50, range=(int(rmin),int(rmax)))\n",
    "        \n",
    "        histo_numbers_reshaped = np.reshape(numbers, (blue.shape[0],blue.shape[1]))\n",
    "        red_means=[]\n",
    "        blue_means=[]\n",
    "        # don't include 0 values in the mean calculations\n",
    "        for i in np.unique(histo_numbers_reshaped)[1:]:\n",
    "            \n",
    "            red_vals = red[histo_numbers_reshaped==i]\n",
    "            blue_vals = blue[histo_numbers_reshaped==i]\n",
    "            #before selecting top 20, reject outliers CHECK IF THIS IS RIGHT\n",
    "            red_vals = reject_outliers_by_med(red_vals)\n",
    "            blue_vals = reject_outliers_by_med(blue_vals)\n",
    "            n = 20\n",
    "            #finds the 20 highest red values and takes mean\n",
    "            red_means.append(\n",
    "                np.mean(\n",
    "                    red_vals[np.argsort(red_vals)[-n:]]\n",
    "                )\n",
    "            )\n",
    "            blue_means.append(\n",
    "                np.mean(\n",
    "                    blue_vals[np.argsort(blue_vals)[-n:]]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if len(np.unique(histo_numbers_reshaped)[1:]) > .5*nbins:\n",
    "            \n",
    "            #followed structure of this example: https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html\n",
    "            model = statsmodels.formula.api.quantreg('reds~blues', {'reds':red_means, 'blues':blue_means})\n",
    "\n",
    "            result = model.fit()\n",
    "\n",
    "            intercept = result.params[0]\n",
    "            slope = result.params[1]\n",
    "            # hardcode if slope too low NEED TO TEST\n",
    "            if slope < 1.5:\n",
    "                slope = 1.5\n",
    "                intercept = np.mean(red_means) - slope*np.mean(blue_means)\n",
    "\n",
    "            return (intercept,slope)\n",
    "        # many cases will have too few bins to compute clear sky line\n",
    "        # assume slope and use available data to compute intercept. this changes results\n",
    "        # substantially, might need to tweak\n",
    "        else: \n",
    "            slope = 1.5\n",
    "            intercept = np.mean(red_means)-slope*np.mean(blue_means)\n",
    "            return (intercept, slope)\n",
    "    else:\n",
    "        # we return nan here to signal that we need to use the \n",
    "        # mean slope and intercept for the good clear skylines\n",
    "        return (np.nan,np.nan) \n",
    "    \n",
    "\n",
    "def compute_hot_series(t_series, rmin, rmax, n_bin=50):\n",
    "    \"\"\"Haze Optimized Transformation (HOT) test\n",
    "    Equation 3 (Zhu and Woodcock, 2012)\n",
    "    Based on the premise that the visible bands for most land surfaces\n",
    "    are highly correlated, but the spectral response to haze and thin cloud\n",
    "    is different between the blue and red wavelengths.\n",
    "    Zhang et al. (2002)\n",
    "    In this implementation, the slope (a) and intercept(b)\n",
    "    of the clear sky line are computed automatically using a bin based approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_series: a 4D array with the band index as the third axis, image index as\n",
    "    the fourth axis (counting from 1st).\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    ndarray: The values of the HOT index for the image, a 3D array\n",
    "    \"\"\"\n",
    "    blues = t_series[:,:,0,:]\n",
    "    reds = t_series[:,:, 2,:]\n",
    "    intercepts_slopes = np.array(\n",
    "        list(map(lambda x: get_clear_skyline(x,rmin,rmax),\n",
    "                np.moveaxis(t_series,3,0)))\n",
    "        )\n",
    "    # assigns slope and intercept if an image is too cloudy (doesn't have 500 pixels in rmin, rmax range)\n",
    "    if np.isnan(intercepts_slopes).all():\n",
    "        # extreme case where no images can get a clear sky line\n",
    "        intercepts_slopes[:,1] = 1.5\n",
    "        intercepts_slopes[:,0] = 0\n",
    "    if np.isnan(intercepts_slopes).any():\n",
    "        # case where some images can't get a clear skyline\n",
    "        intercepts_slopes[:,1][np.isnan(intercepts_slopes[:,1])] = np.nanmean(intercepts_slopes[:,1])\n",
    "        intercepts_slopes[:,0][np.isnan(intercepts_slopes[:,0])] = np.nanmean(intercepts_slopes[:,0])\n",
    "    def helper(blue, red, ba):\n",
    "        b,a = ba\n",
    "        return abs(a*blue - (b+red))/np.sqrt(1+a**2)\n",
    "    # map uses the first axis as the axis to step along\n",
    "    # need to use lambda to use multiple args\n",
    "    hot_t_series = np.array(list(map(lambda x,y,z: helper(x,y,z), \n",
    "                    np.moveaxis(blues,2,0), \n",
    "                    np.moveaxis(reds,2,0), \n",
    "                    intercepts_slopes)))\n",
    "    return hot_t_series, intercepts_slopes\n",
    "\n",
    "def reassign_labels(class_img, cluster_centers, k=3):\n",
    "    \"\"\"Reassigns mask labels of t series\n",
    "    based on magnitude of the cluster centers.\n",
    "    This assumes land will always be less than thin\n",
    "    cloud which will always be less than thick cloud,\n",
    "    in HOT units\"\"\"\n",
    "    idx = np.argsort(cluster_centers.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(k)\n",
    "    return lut[class_img]\n",
    "\n",
    "def fit_predict_reassign(x, km):\n",
    "    \"\"\"returns the reassigned prediction for a single image\"\"\"\n",
    "    model = km.fit(x.reshape((-1,1)))\n",
    "    return reassign_labels(model.labels_.reshape(x.shape), model.cluster_centers_)\n",
    "\n",
    "def create_cloud_masks(hot_t_series):\n",
    "    \"\"\"Runs kmeans with 3 clusters and returns an array\n",
    "    with 3 labels, where the cluster with the smallest\n",
    "    kmeans center is assigned as land (0), then the next to\n",
    "    thin clouds (1), and the next to thick clouds (2)\"\"\"\n",
    "    km = KMeans(n_clusters=3, n_init=10, max_iter=100, tol=1e-4, n_jobs=-1, \n",
    "                      verbose=False, random_state=4)\n",
    "\n",
    "    cloud_masks = np.array(list(map(\n",
    "        lambda x: fit_predict_reassign(x, km),\n",
    "        hot_t_series\n",
    "        )))\n",
    "    return cloud_masks\n",
    "\n",
    "def sample_and_kmeans(hot_t_series, hard_hot=3000000, sample_size=1000000):\n",
    "    \"\"\"Trains a kmeans model on a sample of the time series\n",
    "    and runs prediction on the time series.\n",
    "    A hard coded threshold for the hot index, hard_hot, is\n",
    "    for allowing the kmeans model to capture more variation \n",
    "    throughout the time series. Without it, kmeans is skewed toward\n",
    "    extremely high HOT values and classifies most of the time series\n",
    "    as not cloudy.\"\"\"\n",
    "    \n",
    "    km = KMeans(n_clusters=3, n_init=50, max_iter=100, tol=1e-4, n_jobs=-1, \n",
    "                      verbose=False, random_state=4)\n",
    "\n",
    "    sample_values = np.random.choice(\n",
    "        hot_t_series.flatten()[hot_t_series.flatten()<hard_hot], \n",
    "        size=sample_size).reshape(-1,1)\n",
    "    \n",
    "    fit_result = km.fit(sample_values)\n",
    "    \n",
    "    predicted_series = fit_result.predict(hot_t_series.flatten().reshape(-1,1)).reshape(hot_t_series.shape)\n",
    "    \n",
    "    return reassign_labels(predicted_series, fit_result.cluster_centers_, k=3)\n",
    "\n",
    "def calculate_upper_thresh(hot_t_series, cloud_masks, A_cloud):\n",
    "    \"\"\"Uses temporal refinement as defined by Zhu and Elmer 2018\n",
    "    to catch thin clouds by defining the upper boundary, U for clear \n",
    "    pixels. Later we might want to compute a neighborhood std \n",
    "    through the t_series.\"\"\"\n",
    "    hot_potential_clear = np.array(list(map(\n",
    "        lambda x, y: np.where(x>0, np.nan, y),\n",
    "        cloud_masks, hot_t_series))) # set cloud to nan\n",
    "    hot_potential_cloudy = np.array(list(map(\n",
    "        lambda x, y: np.where(x==0, np.nan, y),\n",
    "        cloud_masks, hot_t_series))) # set non cloud to nan\n",
    "    t_series_std = np.nanstd(hot_potential_clear, axis=0)\n",
    "    t_series_mean = np.nanmean(hot_potential_clear, axis=0)\n",
    "    t_series_min = np.nanmin(hot_potential_clear, axis=0)\n",
    "    t_series_max = np.nanmax(hot_potential_clear, axis=0)\n",
    "    range_arr = t_series_max - t_series_min\n",
    "    \n",
    "    # cloud_series_min can be computed more efficiently using k means centers \n",
    "    # if a single k means model is used\n",
    "    # according to Zhu in personal communciation. This is done in IDL code\n",
    "    \n",
    "    # NRDI (adjust_T in the IDL code) is a problem here because the HOT indices \n",
    "    # vary a lot in the planet images. if we train a kmeans model for each image\n",
    "    # Th_initial will have a very low initial value, if we train one kmeans model\n",
    "    # then the model will produce innacurate initial masks because of extremely high\n",
    "    # HOT values. Need to find a work around.\n",
    "    \n",
    "    # the sticky point is how cloud_series_min is calculated. if it is the minimum\n",
    "    # of all cloudy areas calculated by multiple kmeans models, it is not correct for \n",
    "    # the whole t series\n",
    "    \n",
    "    #calcualting with multiple kmeans\n",
    "    cloud_series_min = np.nanmin(hot_potential_cloudy.flatten(), axis=0)\n",
    "    \n",
    "    NRDI = (cloud_series_min - range_arr)/(cloud_series_min + range_arr)\n",
    "    upper_thresh_arr = t_series_mean + (A_cloud+NRDI)*t_series_std\n",
    "    \n",
    "    return (upper_thresh_arr, hot_potential_clear, hot_potential_cloudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATSA_IDL_DIR = os.path.join(ATSA_DIR, \"planet-idlatsa-test\")\n",
    "ATSA_IDL_INTER_DIR = os.path.join(ATSA_IDL_DIR, \"intermediates-tif\")\n",
    "\n",
    "os.listdir(ATSA_IDL_INTER_DIR)\n",
    "\n",
    "hot_series_idl = ski.io.imread(os.path.join(ATSA_IDL_INTER_DIR,\"hot_image.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_t_series_py, intercepts_slopes = compute_hot_series(t_series, rmin, rmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(hot_series_idl, 10, \"idl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5\n",
    "print(hot_t_series_py[p].max())\n",
    "\n",
    "print(hot_t_series_py[p].min())\n",
    "\n",
    "print(hot_series_idl[p].max())\n",
    "\n",
    "print(hot_series_idl[p].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(hot_t_series_py, 10, \"py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_upper_thresh(hot_t_series, upper_thresh_arr, cloud_masks, hot_potential_clear, hot_potential_cloudy):\n",
    "    \"\"\"Applies the masking logic to refine the initial cloud\n",
    "    masks from k-means using the global threshold and \n",
    "    upper threshold computed from the time series.\n",
    "    Returns a time series of refined masks.\"\"\"\n",
    "    \n",
    "    cloud_series_mean_global = np.nanmean(hot_potential_cloudy.flatten(), axis=0)\n",
    "    cloud_series_std_global = np.nanstd(hot_potential_cloudy.flatten(), axis=0)\n",
    "    global_cloud_thresh = cloud_series_mean_global - 1.0*cloud_series_std_global\n",
    "    # 0 is where hot is below upper threshold, 1 is above\n",
    "    upper_masks = np.where(hot_t_series < upper_thresh_arr, 0, 1)\n",
    "    \n",
    "    refined_masks = np.where((upper_masks < 1) & cloud_masks, 0, 1)\n",
    "    # too many pixels are above upper threshold right now so this sets too many non cloud to cloud\n",
    "    refined_masks = np.where((upper_masks > 0) & ~cloud_masks, 1, refined_masks)\n",
    "    \n",
    "    global_thresh_arr = np.ones(refined_masks.shape)*global_cloud_thresh\n",
    "    \n",
    "    refined_masks = np.where(hot_t_series > global_cloud_thresh, 1, refined_masks)\n",
    "    \n",
    "    return refined_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do to profile bad time series step\n",
    "\n",
    "check range of minimum cloudy hot values and compare to single kmeans model HOT value\n",
    "\n",
    "compare idl Landsat results to pyatsa partial landsat results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_t_series, intercepts_slopes = compute_hot_series(t_series, rmin, rmax)\n",
    "cloud_masks = create_cloud_masks(hot_t_series) # this runs very slow since I fit a new kmeans model to each image\n",
    "cloud_masks = np.array(list(map(\n",
    "    lambda x: ski.morphology.binary_opening(x,[[1,1,1],[1,1,1],[1,1,1]]), \n",
    "    cloud_masks))) # helpfully converts clouds to a binary mask and opens at same time\n",
    "\n",
    "upper_thresh_arr, hot_potential_clear, hot_potential_cloudy = calculate_upper_thresh(hot_t_series, cloud_masks, A_cloud)\n",
    "\n",
    "refined_masks = apply_upper_thresh(hot_t_series, upper_thresh_arr, cloud_masks, hot_potential_clear, hot_potential_cloudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_masks = sample_and_kmeans(hot_t_series, hard_hot=3000000, sample_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_upper_thresh_arr, sample_hot_potential_clear, sample_hot_potential_cloudy = calculate_upper_thresh(hot_t_series, sample_masks, A_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.nanmin(sample_hot_potential_cloudy, axis=(1,2)), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.nanmin(sample_hot_potential_clear, axis=(1,2)), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking mins\n",
    "\n",
    "plt.hist(np.nanmin(hot_potential_cloudy, axis=(1,2)), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.nanmin(hot_potential_clear, axis=(1,2)), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_series_mean_global = np.nanmean(hot_potential_cloudy.flatten(), axis=0)\n",
    "cloud_series_std_global = np.nanstd(hot_potential_cloudy.flatten(), axis=0)\n",
    "global_cloud_thresh = cloud_series_mean_global - 1.0*cloud_series_std_global\n",
    "# 0 is where hot is below upper threshold, 1 is above\n",
    "upper_masks = np.where(hot_t_series < upper_thresh_arr, 0, 1)\n",
    "\n",
    "refined_masks_1 = np.where((upper_masks < 1) & cloud_masks, 0, 1)\n",
    "# too many pixels are above upper threshold right now so this sets too many non cloud to cloud\n",
    "refined_masks_2 = np.where((upper_masks > 0) & ~cloud_masks, 1, refined_masks_1)\n",
    "\n",
    "global_thresh_arr = np.ones(refined_masks_1.shape)*global_cloud_thresh\n",
    "\n",
    "refined_masks_3 = np.where(hot_t_series > global_cloud_thresh, 1, refined_masks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(upper_thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(np.moveaxis(t_series, 3, 0)[:,:,:,0], 10, \"Cloud Masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes on IDL code follow the values to determine what conditions to use in python\n",
    "\n",
    "The mask values are as follows (everything starts as 1) and water mask is 0 value where water\n",
    "\n",
    "* 3 - background/SLC errors, missing data\n",
    "* 2 - cloud (see lines 365 through 378)\n",
    "* 1 - clear land (see lines 323 through 331, where idl returns 1 or 0 from ge condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling why time series checking produces vertical artifacts and worse masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_potential_clear = np.array(list(map(\n",
    "    lambda x, y: np.where(x>0, np.nan, y),\n",
    "    cloud_masks, hot_t_series))) # set cloud to nan\n",
    "hot_potential_cloudy = np.array(list(map(\n",
    "    lambda x, y: np.where(x==0, np.nan, y),\n",
    "    cloud_masks, hot_t_series))) # set non cloud to nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_series_std = np.nanstd(hot_potential_clear, axis=0)\n",
    "t_series_mean = np.nanmean(hot_potential_clear, axis=0)\n",
    "t_series_min = np.nanmin(hot_potential_clear, axis=0)\n",
    "t_series_max = np.nanmax(hot_potential_clear, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"std\")\n",
    "ski.io.imshow(t_series_std)\n",
    "plt.figure()\n",
    "plt.title(\"mean\")\n",
    "ski.io.imshow(t_series_mean)\n",
    "plt.figure()\n",
    "plt.title(\"min\")\n",
    "ski.io.imshow(t_series_min)\n",
    "plt.figure()\n",
    "plt.title(\"max\")\n",
    "ski.io.imshow(t_series_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_arr = t_series_max - t_series_min\n",
    "cloud_series_min = np.nanmin(np.nanmin(hot_potential_cloudy, axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(np.nanmin(hot_potential_cloudy, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(np.nan_to_num(hot_potential_cloudy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(range_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idl code used mean, paper states minimum should be used\n",
    "NRDI = (cloud_series_min - range_arr)/(cloud_series_min + range_arr)\n",
    "upper_bound_arr = t_series_mean + (A_cloud+NRDI)*t_series_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(NRDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(upper_bound_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(hot_t_series[3,:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(np.where(hot_t_series[3]<upper_bound_arr, hot_t_series[3], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_mask_updated = np.array(list(map(\n",
    "    lambda x: np.where(x < upper_bound_arr, 0, 1), hot_t_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_masks = sample_and_kmeans(hot_t_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_t_series_masked = np.array(list(map(\n",
    "    lambda x, y: np.where(x, np.nan, y),\n",
    "    cloud_masks, hot_t_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.io.imshow(np.nan_to_num(hot_t_series_masked[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making presentation figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_imgs_with_meta(img_paths, meta_paths, DIR):\n",
    "    imgs_with_meta = []\n",
    "    for im in list(map(os.path.basename, sorted(img_paths))):\n",
    "        for meta in list(map(os.path.basename, sorted(meta_paths))):\n",
    "            if meta[0:15] == im[0:15]:\n",
    "                imgs_with_meta.append(im)\n",
    "    return [os.path.join(DIR,name) for name in imgs_with_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udm_pattern = \"/home/rave/cloud-free-planet/notebooks/jan-september/*udm*.tif\"\n",
    "udm_paths = glob.glob(udm_pattern)\n",
    "udm_paths = sorted(udm_paths)\n",
    "meta_pattern = \"/home/rave/cloud-free-planet/notebooks/jan-september/*metadata.json\"\n",
    "meta_paths = glob.glob(meta_pattern)\n",
    "meta_paths = sorted(meta_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udm_with_meta = keep_imgs_with_meta(udm_paths, meta_paths, \"/home/rave/cloud-free-planet/notebooks/jan-september/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [ski.io.imread(path) for path in udm_with_meta]\n",
    "udms_stacked = reshape_as_raster(np.dstack(arrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udms_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "i = 0\n",
    "while i<20:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"HOT Index Mask (Partial ATSA)\")\n",
    "#     plt.title(\"Intercept:\"+str(intercepts_slopes[i][0])+\n",
    "#               \" and Slope:\"+str(intercepts_slopes[i][1]))\n",
    "    ski.io.imshow(cloud_masks[i,:,:])\n",
    "    plt.savefig(str(i)+\"atsa-k.png\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"UDM Mask (Planet)\")\n",
    "    ski.io.imshow(udms_stacked[i,:,:])\n",
    "    plt.savefig(str(i)+\"udm.png\")\n",
    "#     ski.io.imshow(hot_t_series[i,:,:])\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"True Color\")\n",
    "    ski.io.imshow(reorder_to_rgb(t_series[:,:,:,i]))\n",
    "    plt.savefig(str(i)+\"true-color.png\")\n",
    "    \n",
    "    i+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo]",
   "language": "python",
   "name": "conda-env-geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
